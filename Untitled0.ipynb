{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChenAndy-7/Professor-Review-Rating-Predictor/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MufZFNiQYTqU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Disable Weights & Biases inside Colab (no external logging)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Install required libraries (most are already present in Colab, but this is safe)\n",
        "!pip install planetterp torch transformers pandas numpy matplotlib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import planetterp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    mean_absolute_error,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "_LAYHHHDY1m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prof list\n",
        "professors = [\n",
        "    \"Hatice Sahinoglu\",\n",
        "    \"Larry Herman\",\n",
        "    \"Ilchul Yoon\",\n",
        "    \"Clyde Kruskal\",\n",
        "    \"Paul Kline\",\n",
        "    \"Pedram Sadeghian\",\n",
        "]\n",
        "\n",
        "all_reviews = []\n",
        "#getting the reviews for all the professors\n",
        "for prof_name in professors:\n",
        "    try:\n",
        "        prof = planetterp.professor(name=prof_name, reviews=True)\n",
        "        reviews = prof.get(\"reviews\", [])\n",
        "        for review in reviews:\n",
        "            all_reviews.append({\n",
        "                \"professor\": prof_name,\n",
        "                \"review\": review.get(\"review\", \"\"),\n",
        "                \"rating\": review.get(\"rating\", 0),\n",
        "            })\n",
        "        print(f\"Fetched {len(reviews)} reviews for {prof_name}\")\n",
        "    except Exception as e:\n",
        "        print(\"error with fetching reviews\")\n",
        "\n",
        "df = pd.DataFrame(all_reviews)\n",
        "print(f\"\\nTotal reviews collected: {len(df)}\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "oK2XoAz7Y4AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data cleansing step:\n",
        "# Remove empty or very short reviews and invalid ratings\n",
        "df = df[df[\"review\"].notna()]\n",
        "df = df[df[\"review\"].str.len() > 50]   # at least 50 characters\n",
        "\n",
        "print(f\"Clean dataset: {len(df)} reviews\\n\")\n",
        "\n",
        "print(\"Rating distribution\")\n",
        "print(df[\"rating\"].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "ZJR4FL9mY6tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ratings (1 to 5) to labels (0 to 4) for the model\n",
        "df[\"label\"] = df[\"rating\"] - 1\n",
        "\n",
        "#main issue with the dataset was that 1 and 5s were much more likely to appear\n",
        "#students that typically leave reviews are typically really happy or really upset\n",
        "#with the professor so I balance the dataset by upsampling minority classes\n",
        "df_bal = df.copy()\n",
        "groups = [g for _, g in df_bal.groupby(\"label\")]\n",
        "max_size = max(len(g) for g in groups)\n",
        "\n",
        "balanced_groups = [\n",
        "    resample(\n",
        "        g,\n",
        "        replace=True,\n",
        "        n_samples=max_size,\n",
        "    )\n",
        "    for g in groups\n",
        "]\n",
        "#combined all the upsampled class's, shuffled the dataset to ensure a final balanced\n",
        "#dataset so that the DS is randomized b4 splitting\n",
        "df_bal = pd.concat(balanced_groups).sample(\n",
        "    frac=1\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(df_bal[\"label\"].value_counts().sort_index())\n",
        "\n",
        "#splitting dataset\n",
        "train_df, temp_df = train_test_split(\n",
        "    df_bal,\n",
        "    test_size=0.2,\n",
        "    stratify=df_bal[\"label\"]\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_df[\"label\"]\n",
        ")\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "y0BtxX56ZH0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-trained model used\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "\n",
        "# Load tokenizer and data collator\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#made a class for function usuability later on\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    #look at specific reviews and the stars they gave\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.data.loc[idx, \"review\"])\n",
        "        label = int(self.data.loc[idx, \"label\"])\n",
        "\n",
        "    #tokenize the review\n",
        "        encoding = self.tokenizer(\n",
        "            review,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ReviewDataset(train_df, tokenizer)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer)\n",
        "\n",
        "print(\"Datasets created successfully.\")\n",
        "print(f\"Sample encoding shape: {train_dataset[0]['input_ids'].shape}\")\n"
      ],
      "metadata": {
        "id": "JWSzT2RQZKD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model for sequence classification with 5 labels (0 to 4 -> 1 to 5 stars)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=5,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2lGKcW5XZQxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "\n",
        "    preds_stars = preds + 1\n",
        "    labels_stars = labels + 1\n",
        "    mae = mean_absolute_error(labels_stars, preds_stars)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"mae\": mae\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "#uses class_weights instead of the normal crossEntropyLoss\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **e_args):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "#my hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,     #ran with > 3 epochs but had diminishing returns so\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,     #tweaked LR a lot\n",
        "    weight_decay=0.015,     #also tweaked this\n",
        "    label_smoothing_factor=0.1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"args set\")\n"
      ],
      "metadata": {
        "id": "f-LsXbryZRza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer with early stopping on validation accuracy\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nnzRj7-AZTjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(cm)  # default colormap\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "# Add numbers to each cell\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, str(cm[i, j]),\n",
        "                 ha='center', va='center')\n",
        "\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a7yZ_V_zymlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}